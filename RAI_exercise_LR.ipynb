{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# Introduction\n",
    "This notebook is part of a machine learning for healthcare exercise, focusing on using the Responsible AI (RAI) package to enhance clinical decision-making. The toolkit will be used to analyze diabetes progression data, with two key objectives:\n",
    "\n",
    "1. Analyze Errors and Explore Interpretability of Models. Run Interpret-Community's 'explain_model' globally to generate model explanations and Visualize model errors and global explanations with the Error Analysis visualization dashboard\n",
    "\n",
    "2. Plan real-world action through counterfactual and causal analysis: By leveraging counterfactual example analysis and causal inferencing, we will explore decision-making strategies based on diabetes progression data to understand potential treatment paths and their impacts\n",
    "\n",
    "3. Assess disease progression predictions: A regression model trained on diabetes progression data will be assessed, examining its performance in predicting disease progression to inform clinical decisions\n",
    "\n",
    "**The goal is for you to provide non-trivial insights for clinical decision making, leveraging machine learning, paired with responsible AI tools, to improve patients outcomes in the healthcare contexts.**\n",
    "\n",
    "Based on notebooks from the [Responsible AI toolkit](https://github.com/microsoft/responsible-ai-toolbox/tree/main/notebooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from interpret.ext.blackbox import TabularExplainer\n",
    "from raiwidgets import ErrorAnalysisDashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the [breast cancer diagnosis data](https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-dataset) and specify the different types of features. Then, clean it and put it into a DataFrame with named columns.\n",
    "\n",
    "Data Set Characteristics:\n",
    "\n",
    "Number of Instances: 569\n",
    "\n",
    "Number of Attributes: 30 numeric, predictive attributes and the class\n",
    "\n",
    "Attribute Information:\n",
    "- radius (mean of distances from center to points on the perimeter)\n",
    "- texture (standard deviation of gray-scale values)\n",
    "- perimeter\n",
    "- area\n",
    "- smoothness (local variation in radius lengths)\n",
    "- compactness (perimeter^2 / area - 1.0)\n",
    "- concavity (severity of concave portions of the contour)\n",
    "- concave points (number of concave portions of the contour)\n",
    "- symmetry\n",
    "- fractal dimension (“coastline approximation” - 1)\n",
    "\n",
    "The mean, standard error, and “worst” or largest (mean of the three worst/largest values) of these features were computed for each image, resulting in 30 features. For instance, field 0 is Mean Radius, field 10 is Radius SE, field 20 is Worst Radius.\n",
    "\n",
    "class:\n",
    "- WDBC-Malignant (\"1\")\n",
    "- WDBC-Benign (\"0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Tumor Image](img\\tumor.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "breast_cancer_data = load_breast_cancer()\n",
    "classes = breast_cancer_data.target_names.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(breast_cancer_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sklearn.datasets.load_breast_cancer()\n",
    "target_feature = 'y'\n",
    "continuous_features = data.feature_names\n",
    "data_df = pd.DataFrame(data.data, columns=data.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Model\n",
    "After loading and cleaning the data, split the datapoints into training and test sets. Assemble separate datasets for the full sample and the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data_df, data.target, test_size=0.2, random_state=7)\n",
    "\n",
    "# Creating train_data and test_data for RAIInsights\n",
    "train_data = X_train.copy()\n",
    "test_data = X_test.copy()\n",
    "train_data[target_feature] = y_train\n",
    "test_data[target_feature] = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a LR (Logistic Regressor) classifier on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Logistic Regression model\n",
    "model = LogisticRegression(random_state=0, max_iter=10000)\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a RF (Random-Forest) classifier on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = RandomForestClassifier(random_state=0)\n",
    "# model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain predictions on your local machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Using SHAP TabularExplainer\n",
    "explainer = TabularExplainer(model,\n",
    "                             X_train,\n",
    "                             features=X_train.columns,\n",
    "                             classes=classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate global explanations\n",
    "Explain overall model predictions (global explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Passing in test dataset for evaluation examples - note it must be a representative sample of the original data\n",
    "# X_train can be passed as well, but with more examples explanations will take longer although they may be more accurate\n",
    "global_explanation = explainer.explain_global(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize\n",
    "Load the interpretability visualization dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from raiwidgets import ExplanationDashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ExplanationDashboard(\n",
    "    global_explanation, \n",
    "    model, \n",
    "    dataset=X_test, \n",
    "    true_y=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model and Data Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from raiwidgets import ResponsibleAIDashboard\n",
    "from responsibleai import RAIInsights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Optional] Defining an \"identity_feature\". An identity feature in Microsoft's Responsible AI Toolkit helps detect and mitigate bias in sensitive attributes like race, gender, and age for fairness. \n",
    "\n",
    "Marking a feature as an identity feature can trigger specific analyses within the toolkit, such as:\n",
    "\n",
    "- Fairness assessment: Evaluating if certain identity groups are disproportionately affected by model decisions\n",
    "- Bias detection: Highlighting patterns of bias in model performance across identity groups\n",
    "- Mitigating unfairness: Providing tools to adjust or retrain models to improve fairness with respect to the identity features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from responsibleai.feature_metadata import FeatureMetadata\n",
    "# Add 's1' as an identity feature\n",
    "feature_metadata = FeatureMetadata(\n",
    "    identity_feature_name=None, \n",
    "    categorical_features=[], \n",
    "    dropped_features=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAIInsights accepts the model, the train dataset, the test dataset, the target feature string and the task type string as its arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rai_insights = RAIInsights(\n",
    "    model, \n",
    "    train_data, \n",
    "    test_data, \n",
    "    target_feature, \n",
    "    'classification',\n",
    "    feature_metadata=feature_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the components of the toolbox that are focused on **Model assessment and Decision-making.**\n",
    "\n",
    "- Explanations (.explainer): Clarifies how model features influence predictions using algorithms like SHAP or LIME\n",
    "- Error Analysis (.error_analysis): Identifies subgroups of data where the model makes the most mistakes, helping with troubleshooting\n",
    "- Counterfactuals (.counterfactual): Generates \"what-if\" scenarios, showing how feature changes affect outcomes\n",
    "- Causal Inference (.causal): Analyzes whether certain features (e.g., bmi, bp) directly cause changes in the outcome\n",
    "    - For potential \"treatments\", we'd use the causal inference component to understand the impact of changing certain features on the outcome:\n",
    "    - mean_radius (mean of distances from the center to the boundary of the nuclei)\n",
    "    - mean_texture (standard deviation of gray-scale values)\n",
    "    - mean_perimeter (average perimeter of the cell nuclei)\n",
    "    - mean_area (mean area of the cell nuclei)\n",
    "    - mean_smoothness (mean local variation in radius lengths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpretability\n",
    "rai_insights.explainer.add()\n",
    "# Error Analysis\n",
    "rai_insights.error_analysis.add()\n",
    "# Counterfactuals: accepts total number of counterfactuals to generate, the range that their label should fall under, \n",
    "# and a list of strings of categorical feature names\n",
    "rai_insights.counterfactual.add(total_CFs=10, desired_class=\"opposite\")\n",
    "# Causal Inference: determines causation between features\n",
    "rai_insights.causal.add(\n",
    "    treatment_features=[\n",
    "        'mean radius',\n",
    "        'mean texture',\n",
    "        'mean perimeter', \n",
    "        'mean smoothness',\n",
    "        'mean area'\n",
    "        ]  # Example features\n",
    "    )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all the desired components have been loaded, we compute insights on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rai_insights.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize and explore the model insights. Use the resulting widget or follow the link to view this in a new tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ResponsibleAIDashboard(rai_insights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)\n",
    "ErrorAnalysisDashboard(dataset=X_test, true_y=y_test,\n",
    "                       features=continuous_features, pred_y=predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RAI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
